
# 主要工作流程

1. 先从资料库中*检索*相关内容
2. 再给予这些内容*生成*用户所需的答案

RAG 是目前最常用的AI问答方案之一. 常用于企业内部知识助手和智能客服
- 针对公司的产品

![[Pasted image 20250810210831.png]]


## LLM 是核心

LLM 可以看作智能产品手册的大脑, 但是只有大脑是不够的, 还需要公司特有的产品手册。

### 不要把整个文档直接丢给模型!!!

如果产品手册字数特别多, 超过了LLM所能够接受的上下文限制, 则会带来以下问题:
1. 模型无法读取所有内容
	- 模型只能存取一定量的信息, 即上下文窗口, 一旦手册的字数超过了模型的上下文窗口, 那么模型就会读了后面忘记前面 --> 无法保障回答的准确率 
2. 模型的推理成本提高
	- 输入越多, 则成本越高, 模型每次回答问题的时候, 都要带上整个产品手册的内容 (tokens量大)
3. 推理速度受限
	- 输入越多, 那么模型需要理解的内容就越多, 模型的输出速度也越慢

### 只把文档中和**问题**相关的内容发给LLM是个好主意!

#### 基本流程

![[Pasted image 20250810211908.png]]

首先RAG会将文档切分为多个片段, 当用户提出问题后, RAG会在所有片段中寻找相关内容, 例如一个文档被划分为上百个片段, 其中只有 3  个片段真正和用户的问题相关。RAG则会将这三个片段和用户的问题一起发给LLM, 这样模型就只会感知这三个相关的片段而不是整个文档。

![[Pasted image 20250810212003.png]]

**注意**:
- 遍历所有片段不会影响模型的速度, 因为LLM不参与遍历, 而是使用向量数据库快速定位问题
- 使用 embedding 进行***相似度检索***


#### 详细流程
RAG 的完整工作流可以分为两步:
1. 准备 (提问前)
	- 准备好相关的文档并完成相应的预处理
	- 包含***分片和索引***两个环节
	- ![[Pasted image 20250810221504.png]]
1. 回答 (提问后)
	- ***召回*** --> ***重排*** --> ***生成***
	- ![[Pasted image 20250810221614.png]]

##### 分片

方式很多, 可以
- 按照字数划分chunks, 每1000字算一个片段
- 按段落分, 每个段落是一个片段
- 按照章节分, 文档的每个章节算一个片段
- 按页码分
- ...


##### 索引
尽管只有两个步骤, 但是这两个步骤所包含的信息量是巨大的, 需要理解什么是 Embedding, 向量数据库

**索引:**
通过 Embedding 将每个片段文本转化为向量, 并将*片段文本*和对应的*向量*都存储在向量数据库的过程。--> 遍历所有的片段

###### 1. 通过 *Embedding* 将片段文本转换为向量

每个向量都有大小和方向

每个向量也都有维度, 即数组中数字的个数。RAG 所用到的向量维度通常比较大, 几百上千。一般来说, 维度越大则其所包含的信息就越丰富, 利用这些向量进行各种工作的可靠性也就越强。

![[Pasted image 20250810214230.png]]


**Embedding:**
将文本转化为向量的过程
距离近的向量的相似性更大 --> 含义相近的文本在经过 Embedding 后对应的向量也就更接近
![[Pasted image 20250810214534.png]]

当用户询问马克喜欢吃什么的时候, 我们可以将这个问题进行 Embedding转化为向量, 然后根据向量的相似度, 把和这个问题相关的文本也找出来:
![[Pasted image 20250810214756.png]]
最后我们可以将相似的文本和用户的问题一起提交给 LLM , 

Embedding 是通过模型来完成的, 注意这里的模型并不是常见的 GPT4, Deepseek等模型, 而是专门的 Embedding 模型。

Embedding 模型的评测网站:
![[Pasted image 20250810215018.png]]


###### 2. 将*片段文本*和*片段向量*存入向量数据库中

向量数据库:
用于存储和查询向量的数据库, 专门为存储向量进行了很多优化, 并且提供了计算向量相似度等相关的函数。

Embedding 后的向量存储在 向量数据库中。

原始文本和 Embedding 后的向量都要存储在向量数据库中, 只有这样我们才能在通过向量相似度查询出相似的向量之后, 把对应的原始文本也抽取出来。

![[Pasted image 20250810215239.png]]

我们最终需要的还是**原始的文本**, 向量只是一个中间结果。--> 一般的向量数据库表格中至少都会有两列内容。
![[Pasted image 20250810215435.png]] 


##### 召回
搜索向量数据库中和用户问题相关的片段

召回从用户提出的问题开始:

![[Pasted image 20250810215846.png]]

召回的结果就是 10 个与用户问题相关的片段 , 是通过计算向量相似度完成的。
RAG 直接用 embedding 计算相似度, 不同于 Transformer 中用词表的每个token的概率

![[Pasted image 20250810220120.png]]

将用户问题向量和每个片段对应的向量逐一计算相似度, 结果写进右侧表格。然后对向量相似度表格进行排序, 取前 n 个最大值。
--> **这样就得到了与用户问题最相关的 n 个片段, 这些片段会在重排阶段继续处理**

计算相似度的常用方法:
- 余弦想速度
- 欧氏距离
- 点积(几何方法)

##### 重排 - 重新排序

从 10 份与用户问题最相关的片段中再挑选 3 份与用户问题最相似的作为结果。

> [!NOTE]
> 看似同样的事情在召回和重排中做了两次, 但是这是为了保证效果更好。如果仅仅使用召回挑选3份相关片段, 则可能出现遗漏
> 此外, 在召回和重排阶段所使用的相似度计算逻辑是不同的, 是效率与精度的权衡

--> 召回可以看作粗选, 而重排则是精选


召回可以在短时间内对上千条片段的相似度数值进行计算并选出10个最高的。而重排则是使用了 cross encoder 模型计算每个片段与用户问题的相似度。

![[Pasted image 20250810221251.png]]

##### 生成
![[Pasted image 20250810221355.png]]






# python 实战
main.ipynb
![[Pasted image 20250810223353.png]]


单元格中的命令是建立一个内存型的数据库, 紫色则是写入硬盘文件中。
![[Pasted image 20250810223826.png]]
> Gemini 2.5 flash 是免费的 










